

\section{ANTLR 4}
We have based part of our Analyzer on ANTLR 4 parser generator. ANLTR 4 implements Adaptive $LL(*)$ \cite{parr2014adaptive} parsing strategy.

\subsection{Adaptive $LL(*)$ parsing strategy}
Adaptive $LL(*)$ (or short $ALL(*)$) parsing strategy is a combination of simple, efficient and predictable top-down $LL(k)$ parsing strategy with power of $GLR$ which can handle non-deterministic and ambiguous grammars. 
Authors move the grammar analysis to parse-time. This lets $ALL(*)$ handle any non-left-recursive context-free grammar rules and for efficiency it caches analysis results in lookahead DFA.

As a possible downside of $ALL(*)$ could be considered theoretical time complexity. Parsing of $n$ symbols takes $O(n^4)$ in theory. In practice, however, $ALL(*)$ seems to outperform other parsers by order of magnitude.

Despite the theoretical $O(n^4)$ time complexity, it appears that the $ALL(*)$ behaves linear on most of the code, with no unpredictable performance or large footprint in practice. In order to support this, authors investigate the parse time vs file size for languages \texttt{C}, \texttt{Verilog}, \texttt{Erlang} and \texttt{Lua} files. They found very strong evidence of linearity on all tested languages (see the original paper for details).

\subsection{ANTLR 4 pipeline}

ANTLR 4, as any other conventional parser generator, processes the inputted code as follows: (1) breaks down the source string into tokens using \textit{lexer} (2) \textit{parser} build parse trees. 

This pipeline is in ANTLR 4 breaked into following classes: 

\begin{description}
	\item[\texttt{CharStream}] represents input code,
	\item[\texttt{Lexer}] breaks the inputted code into tokens,
	\item[\texttt{Token}] token representation that includes important information like token type, position in code or the actual text,
	\item[\texttt{Parser}] builds parse trees,
	\item[\texttt{TokenStream}] connects the lexer and parser.
\end{description}

Overview of the described pipeline can be seen in \cref{antlr_pipeline}.

\begin{figure}[H]
	\centering
	\includegraphics[width=6cm]{img/antlr_pipeline}
	\label{antlr_pipeline}
	\caption{ANTLR 4 pipeline overview. Taken from \cite{parr2013definitive}.}
\end{figure}

\subsection{Naming convetion remark}
ANTLR 4 uses Java naming convention. In our code, we follow in general \XX{Google C++ Style Guide}\footnote{\url{https://google.github.io/styleguide/cppguide.html}}. 

When implementing ANTLR 4 classes, we remain consistent with the naming convention used in our project. As a benefit, the overriden functions are better visible in code.

\section{Lexer}

Lexer's task is to read source string and break it into tokens --- small pieces of text with special meaning. The most important properties of the lexer:
\begin{itemize}
	\item each token has location in the source text,
	\item has the ability to check whether all characters are valid in the HLASM source,
	\item has the ability to jump in the source file backward and forward if necessary (for implementation of instructions like AGO and AIF). Because of this, it is not possible to use any standard lexing tool and the lexer has to be implemented from scratch.
\end{itemize}

As already mentioned, we designed a custom lexer for HLASM. We have a number of reasons to do so. HLASM language is complex, it was first introduced several decades ago and the language was during this long time subjected to development. This made the HLASM language complex. Also, it contains some aggressive features as for example \texttt{AREAD} or \texttt{COPY} that are able to alter the source code at parse time.

Conventional lexing tools are most often based on regular expressions. As commented above, there are several complications that one must consider designing lexer for this particular language. A regular expression-based lexer would be too difficult or even impossible to design\footnote{One could match separate characters from input and let the parser or semantic analysis deal with some of the described problems. Obviously, this drastic solution would cost performance, as parsers are usually more performance demanding.}.

\subsection{Encodings}
Source code encodings differ for the used libraries. All strings are encoded in \texttt{UTF} as follows:

\begin{description}
	\item[\texttt{UTF-8}] LSP string encoding,
	\item[\texttt{UTF-16}] offsets (positions in source code) in LSP,
	\item[\texttt{UTF-32}] ANTLR 4 source code representation.
\end{description}

\subsection{Architecture}
Beside of the custom lexer, we altered ANTLR's classes \texttt{Token}, \texttt{TokenFactory} and \texttt{ANTLRInputStream}. The reason is to add custom attributes to token that are vital for later stages (parsing, semantic analyses, etc.). Lexer functionality is implemented in following classes (see \cref{lexer_arch}):

\begin{figure}[H]
	\centering
	\includegraphics{img/lexer_arch}
	\label{lexer_arch}
	\caption{Lexer architecture overview. Note, there are two \texttt{input\_source}s and there are many \texttt{token}s generated.}
\end{figure}

\begin{description}
	
		\item[\texttt{token}] implements ANTLR's class \texttt{Token} and extends it by adding properties important for location of the token within the input stream. As the LSP protocol works with offsets encoded in \texttt{UTF-16} and ANTLR 4 works with \texttt{UTF-32} encoding, we add attributes for \texttt{UTF-16} positions, too.
		
		Token does not carry the actual text from original source, but rather references to the position in code (unlike \texttt{CommonToken}). Note, that the position of a token is vital for further analysis.
		
		\XX{TODO: token types description}
		
		\item[\texttt{token\_factory}] produces tokens of previously described custom type \texttt{token}.
		
		\item[\texttt{input\_source}] implements \texttt{ANTLRInputStream} which encapsulates source code. This implementation adds API for resetting, rewinding and rewriting input. 
		
		Beware of the usage of \texttt{UTF} encodings: \texttt{\_data} (source code string) and positions/indices in API are in \texttt{UTF-32}; \texttt{getText} returns \texttt{UTF-8} string.
		
		\item[\texttt{lexer}] is based on ANTLR's \texttt{TokenSource} class. As most lexers, it is also in principle a finite state machine. Most important difference compared to conventional FSMs and other lexers is added communication interface that connects the parser and the instruction interpretator with the lexer. Unusual is also input rewinding (to support \texttt{AREAD}, etc.), lexing from parallel sources (\texttt{AINSERT} buffer) and some helper API for subsequent processing stages.
		
		Important functions:
		
		\begin{description}
			\item[\texttt{nextToken()}] implements main functionality: lexes and emits tokens. Before lexing, the function uses right input stream (either the source code or \texttt{AINSERT} buffer if not empty). After choosing right input source, the token is lexed. HLASM introduces \textit{continuation} symbol (an arbitrary non-blank symbol at column 72 by default), that brakes one logical line into two or more lines in code. \texttt{EOLLN} token is introduced to indicate the end of one logical line. Such token is important for further (syntactic and semantic) analysis.
			
			\item[\texttt{create\_token()}] creates token of given type. Position of the token is given by the lexer's internal state. 
			
			\item[\texttt{consume()}] consumes character from the input stream and updates lexer's internal state (used in \texttt{create\_token()}).
			
			\item[\texttt{lex\_tokens()}] lexing of most of the token types.
			
			\item[\texttt{lex\_begin()}] up to certain column, the input can be ignored (can be set in HLASM).
			
			\item[\texttt{lex\_end()}] lexes everything after continuation symbol.
			
			
		\end{description}
		
\end{description}


\subsection{Parser}

Parser component takes the stream of tokens the lexer produces and recognizes HLASM statements according to the syntax. To accomplish this, a parser generator tool Antlr 4 \footnote{\url{https://www.antlr.org}} is used.

The input to Antlr is a grammar (written in antlr-specific language) that specifies the syntax of HLASM language and generates source code (in C++) for a recognizer, which is able to tell whether input source code is valid or not. Moreover, it is possible to assign a piece of code that executes every time a grammar rule is matched by the recognizer to further process the matched piece of code.
